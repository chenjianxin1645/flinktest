apiVersion: v1
kind: ConfigMap
metadata:
  namespace: flink
  name: flink-config
  labels:
    app: flink
data:
  flink-conf.yaml: |+
    jobmanager.rpc.address: flink-jobmanager
    blob.server.port: 6124
    jobmanager.rpc.port: 6123
    taskmanager.rpc.port: 6122
    queryable-state.proxy.ports: 6125
    jobmanager.memory.process.size: 1024m
    kubernetes.jobmanager.cpu: 0.5
    taskmanager.memory.process.size: 1024m
    kubernetes.taskmanager.cpu: 0.5
    taskmanager.numberOfTaskSlots: 2
    parallelism.default: 1
    execution.checkpointing.interval: 60s
    
    kubernetes.cluster-id: flink-k8s-ha
    high-availability: kubernetes
    high-availability.storageDir: hdfs://emr-cluster/flink/recovery
    restart-strategy: fixed-delay
    restart-strategy.fixed-delay.attempts: 10

  log4j-console.properties: |+
    # 如下配置会同时影响用户代码和 Flink 的日志行为
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    rootLogger.appenderRef.rolling.ref = RollingFileAppender

    # 如果你只想改变 Flink 的日志行为则可以取消如下的注释部分
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO

    # 下面几行将公共 libraries 或 connectors 的日志级别保持在 INFO 级别。
    # root logger 的配置不会覆盖此处配置。
    # 你必须手动修改这里的日志级别。
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO

    # 将所有 info 级别的日志输出到 console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # 将所有 info 级别的日志输出到指定的 rolling file
    appender.rolling.name = RollingFileAppender
    appender.rolling.type = RollingFile
    appender.rolling.append = false
    appender.rolling.fileName = ${sys:log.file}
    appender.rolling.filePattern = ${sys:log.file}.%i
    appender.rolling.layout.type = PatternLayout
    appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    appender.rolling.policies.type = Policies
    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    appender.rolling.policies.size.size=100MB
    appender.rolling.strategy.type = DefaultRolloverStrategy
    appender.rolling.strategy.max = 10

    # 关闭 Netty channel handler 中不相关的（错误）警告
    logger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
    
  core-site.xml: |+
    <?xml version="1.0"?>
  <configuration>
  <property>
  <name>hadoop.proxyuser.zhouziheng.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.sqcp.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.xuwenchao.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_asset.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.xinyuexuan.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>ha.failover-controller.new-active.rpc-timeout.ms</name>
  <value>60000</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_pm.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.yxz.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>fs.trash.checkpoint.interval</name>
  <value>30</value>
  </property>
  <property>
  <name>hadoop.proxyuser.flowagent.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hdfs.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.registry.zk.session.timeout.ms</name>
  <value>60000</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_zycp.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.xuwenchao.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.dcq.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.oozie.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>ipc.client.connect.timeout</name>
  <value>20000</value>
  </property>
  <property>
  <name>hadoop.proxyuser.zhouziheng.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>ipc.client.connect.max.retries.on.timeouts</name>
  <value>45</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_pm.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hbase.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hue.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_devm.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.knox.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.liuxianlin.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.livy.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>ipc.client.idlethreshold</name>
  <value>4000</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_sale.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>io.file.buffer.size</name>
  <value>4096</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_zycp.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>io.compression.codec.lzo.class</name>
  <value>com.hadoop.compression.lzo.LzoCodec</value>
  </property>
  <property>
  <name>ipc.client.kill.max</name>
  <value>10</value>
  </property>
  <property>
  <name>io.mapfile.bloom.size</name>
  <value>1048576</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hue.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.liutaisheng.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.xwh.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ytt.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_devm.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_hard.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.zhangw.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.wxf.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.chenxiaojun.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.cy.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.caojun.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.tangg.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hdfs.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.tangg.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.lzl.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ywb.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.livy.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_kf.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.registry.zk.connection.timeout.ms</name>
  <value>15000</value>
  </property>
  <property>
  <name>hadoop.proxyuser.wangxiaojun.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.yzc.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>io.seqfile.compress.blocksize</name>
  <value>1000000</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_admin.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_all_read.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.home</name>
  <value>/usr/lib/hadoop</value>
  </property>
  <property>
  <name>hadoop.proxyuser.flowagent.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_yfld.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hadoop.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_manager.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>ipc.client.connect.max.retries</name>
  <value>10</value>
  </property>
  <property>
  <name>hadoop.security.auth_to_local</name>
  <value>RULE:[1:$1]
  RULE:[2:$1]
  DEFAULT</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ytt.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hjb.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.liuxianlin.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>io.serializations</name>
  <value>org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization</value>
  </property>
  <property>
  <name>hadoop.proxyuser.wangmanlin.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_yfld.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.wxf.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>io.bytes.per.checksum</name>
  <value>512</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_op.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_dm_read.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>net.topology.script.file.name</name>
  <value>/etc/ecm/hadoop-conf/rack-topology.sh</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_manager.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hadoop.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.xym.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.security.authentication.use.has</name>
  <value>true</value>
  </property>
  <property>
  <name>hadoop.proxyuser.fangli.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.util.hash.type</name>
  <value>murmur</value>
  </property>
  <property>
  <name>hadoop.proxyuser.fangli.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_kf.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.wangmanlin.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.lzl.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>file.replication</name>
  <value>1</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_op.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>fs.du.interval</name>
  <value>600000</value>
  </property>
  <property>
  <name>io.seqfile.local.dir</name>
  <value>${hadoop.tmp.dir}/io/local</value>
  </property>
  <property>
  <name>io.compression.codecs</name>
  <value>com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec</value>
  </property>
  <property>
  <name>hadoop.tmp.dir</name>
  <value>/mnt/disk1/hadoop/tmp</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_sale.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.yzc.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hjb.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>net.topology.script.number.args</name>
  <value>1000</value>
  </property>
  <property>
  <name>hadoop.proxyuser.cy.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>ha.health-monitor.check-interval.ms</name>
  <value>1000</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ywb.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>file.blocksize</name>
  <value>67108864</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hyl.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.wangxiaojun.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.chenxiaojun.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.yxz.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.registry.zk.quorum</name>
  <value>localhost:2181</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_all_read.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.sqcp.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.xkj.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_dm_read.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>ha.zookeeper.parent-znode</name>
  <value>/hadoop-ha</value>
  </property>
  <property>
  <name>hadoop.proxyuser.knox.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>fs.trash.interval</name>
  <value>1440</value>
  </property>
  <property>
  <name>hadoop.proxyuser.liutaisheng.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.caojun.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.xkj.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_asset.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>ipc.client.connect.retry.interval</name>
  <value>1000</value>
  </property>
  <property>
  <name>fs.defaultFS</name>
  <value>hdfs://emr-cluster</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_hr.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.yzl.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.oozie.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hyl.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.hbase.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.dcq.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.xwh.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.xym.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>ipc.client.connection.maxidletime</name>
  <value>10000</value>
  </property>
  <property>
  <name>hadoop.proxyuser.zhangw.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>fs.permissions.umask-mode</name>
  <value>026</value>
  </property>
  <property>
  <name>fs.df.interval</name>
  <value>60000</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_admin.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.yzl.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>ha.zookeeper.session-timeout.ms</name>
  <value>60000</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_hard.groups</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.ld_hr.hosts</name>
  <value>*</value>
  </property>
  <property>
  <name>hadoop.proxyuser.xinyuexuan.groups</name>
  <value>*</value>
  </property>
  </configuration>

  hdfs-site.xml: |+
    <?xml version="1.0"?>
  <configuration>
  <property>
  <name>dfs.namenode.avoid.write.stale.datanode</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.namenode.audit.log.debug.cmdlist</name>
  <value/>
  </property>
  <property>
  <name>dfs.client.failover.connection.retries.on.timeouts</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.balancer.max-no-move-interval</name>
  <value>60000</value>
  </property>
  <property>
  <name>hadoop.user.group.metrics.percentiles.intervals</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.kerberos.internal.spnego.principal</name>
  <value>${dfs.web.authentication.kerberos.principal}</value>
  </property>
  <property>
  <name>dfs.webhdfs.rest-csrf.enabled</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.namenode.hosts.provider.classname</name>
  <value>org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager</value>
  </property>
  <property>
  <name>dfs.namenode.name.dir</name>
  <value>file:///mnt/disk1/hdfs/name</value>
  </property>
  <property>
  <name>dfs.journalnode.keytab.file</name>
  <value/>
  </property>
  <property>
  <name>dfs.journalnode.kerberos.principal</name>
  <value/>
  </property>
  <property>
  <name>dfs.client.cache.drop.behind.reads</name>
  <value/>
  </property>
  <property>
  <name>dfs.datanode.plugins</name>
  <value/>
  </property>
  <property>
  <name>ipc.8020.callqueue.impl</name>
  <value>org.apache.hadoop.ipc.FairCallQueue</value>
  </property>
  <property>
  <name>dfs.namenode.lock.detailed-metrics.enabled</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.namenode.acls.enabled</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.datanode.keytab.file</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.xattrs.enabled</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.webhdfs.rest-csrf.methods-to-ignore</name>
  <value>GET,OPTIONS,HEAD,TRACE</value>
  </property>
  <property>
  <name>dfs.namenode.fslock.fair</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.namenode.service.handler.count</name>
  <value>10</value>
  </property>
  <property>
  <name>dfs.client.mmap.cache.size</name>
  <value>256</value>
  </property>
  <property>
  <name>hadoop.fuse.timer.period</name>
  <value>5</value>
  </property>
  <property>
  <name>dfs.ha.zkfc.nn.http.timeout.ms</name>
  <value>20000</value>
  </property>
  <property>
  <name>dfs.namenode.edit.log.autoroll.multiplier.threshold</name>
  <value>2.0</value>
  </property>
  <property>
  <name>dfs.namenode.list.openfiles.num.responses</name>
  <value>1000</value>
  </property>
  <property>
  <name>nfs.allow.insecure.ports</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.webhdfs.ugi.expire.after.access</name>
  <value>600000</value>
  </property>
  <property>
  <name>dfs.datanode.data.dir</name>
  <value>file:///mnt/disk1/hdfs</value>
  </property>
  <property>
  <name>dfs.namenode.http-address</name>
  <value>50070</value>
  </property>
  <property>
  <name>dfs.blockreport.initialDelay</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.datanode.directoryscan.throttle.limit.ms.per.sec</name>
  <value>1000</value>
  </property>
  <property>
  <name>dfs.namenode.support.allow.format</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.encrypt.data.transfer.cipher.suites</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.handler.count</name>
  <value>10</value>
  </property>
  <property>
  <name>dfs.namenode.accesstime.precision</name>
  <value>3600000</value>
  </property>
  <property>
  <name>dfs.namenode.path.based.cache.refresh.interval.ms</name>
  <value>30000</value>
  </property>
  <property>
  <name>dfs.permissions</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.block.replicator.default.local.rack</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.encrypt.data.transfer.algorithm</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.max-lock-hold-to-release-lease-ms</name>
  <value>25</value>
  </property>
  <property>
  <name>dfs.namenode.servicerpc-bind-host</name>
  <value>0.0.0.0</value>
  </property>
  <property>
  <name>dfs.namenode.shared.edits.dir</name>
  <value>qjournal://172.16.220.100:8485;172.16.220.102:8485;172.16.220.103:8485/emr-cluster</value>
  </property>
  <property>
  <name>dfs.datanode.max.locked.memory</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.namenode.resource.checked.volumes</name>
  <value/>
  </property>
  <property>
  <name>fs.oss.impl.disable.cache</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.datanode.du.reserved</name>
  <value>1073741824</value>
  </property>
  <property>
  <name>dfs.namenode.retrycache.expirytime.millis</name>
  <value>600000</value>
  </property>
  <property>
  <name>dfs.image.transfer.bandwidthPerSec</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.client.failover.sleep.base.millis</name>
  <value>500</value>
  </property>
  <property>
  <name>dfs.lock.suppress.warning.interval</name>
  <value>10s</value>
  </property>
  <property>
  <name>dfs.namenode.lifeline.handler.count</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.kerberos.principal</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.replication.max-streams</name>
  <value>100</value>
  </property>
  <property>
  <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.blockreport.intervalMsec</name>
  <value>21600000</value>
  </property>
  <property>
  <name>dfs.namenode.upgrade.domain.factor</name>
  <value>${dfs.replication}</value>
  </property>
  <property>
  <name>dfs.namenode.max.objects</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.namenode.lease-recheck-interval-ms</name>
  <value>2000</value>
  </property>
  <property>
  <name>dfs.journalnode.kerberos.internal.spnego.principal</name>
  <value/>
  </property>
  <property>
  <name>dfs.datanode.imbalance.threshold</name>
  <value>10</value>
  </property>
  <property>
  <name>dfs.namenode.replication.min</name>
  <value>1</value>
  </property>
  <property>
  <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.namenode.avoid.read.stale.datanode</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.replication.max</name>
  <value>512</value>
  </property>
  <property>
  <name>nfs.wtmax</name>
  <value>1048576</value>
  </property>
  <property>
  <name>dfs.user.home.dir.prefix</name>
  <value>/user</value>
  </property>
  <property>
  <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.namenode.checkpoint.txns</name>
  <value>1000000</value>
  </property>
  <property>
  <name>dfs.client.block.write.replace-datanode-on-failure.min-replication</name>
  <value>0</value>
  </property>
  <property>
  <name>nfs.dump.dir</name>
  <value>/tmp/.hdfs-nfs</value>
  </property>
  <property>
  <name>dfs.namenode.safemode.min.datanodes</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.webhdfs.use.ipc.callq</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.namenode.fs-limits.max-xattr-size</name>
  <value>16384</value>
  </property>
  <property>
  <name>dfs.namenode.edits.noeditlogchannelflush</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.client.short.circuit.replica.stale.threshold.ms</name>
  <value>1800000</value>
  </property>
  <property>
  <name>dfs.ha.tail-edits.period</name>
  <value>60</value>
  </property>
  <property>
  <name>nfs.server.port</name>
  <value>2049</value>
  </property>
  <property>
  <name>dfs.balancer.keytab.enabled</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.blockreport.split.threshold</name>
  <value>1000000</value>
  </property>
  <property>
  <name>dfs.namenode.inotify.max.events.per.rpc</name>
  <value>1000</value>
  </property>
  <property>
  <name>dfs.namenode.decommission.interval</name>
  <value>30</value>
  </property>
  <property>
  <name>dfs.namenode.path.based.cache.retry.interval.ms</name>
  <value>30000</value>
  </property>
  <property>
  <name>dfs.namenode.block-placement-policy.default.prefer-local-node</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.client.block.write.locateFollowingBlock.initial.delay.ms</name>
  <value>400</value>
  </property>
  <property>
  <name>dfs.datanode.balance.bandwidthPerSec</name>
  <value>10485760</value>
  </property>
  <property>
  <name>dfs.short.circuit.shared.memory.watcher.interrupt.check.ms</name>
  <value>60000</value>
  </property>
  <property>
  <name>dfs.namenode.reject-unresolved-dn-topology-mapping</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.namenode.invalidate.work.pct.per.iteration</name>
  <value>0.32f</value>
  </property>
  <property>
  <name>dfs.webhdfs.socket.read-timeout</name>
  <value>60s</value>
  </property>
  <property>
  <name>dfs.client.server-defaults.validity.period.ms</name>
  <value>3600000</value>
  </property>
  <property>
  <name>dfs.client.read.shortcircuit.streams.cache.expiry.ms</name>
  <value>300000</value>
  </property>
  <property>
  <name>dfs.balancer.keytab.file</name>
  <value/>
  </property>
  <property>
  <name>dfs.datanode.transfer.socket.recv.buffer.size</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.namenode.resource.check.interval</name>
  <value>5000</value>
  </property>
  <property>
  <name>dfs.namenode.replication.work.multiplier.per.iteration</name>
  <value>10</value>
  </property>
  <property>
  <name>dfs.datanode.block-pinning.enabled</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.nameservice.id</name>
  <value>emr-cluster</value>
  </property>
  <property>
  <name>smartdata.cache.buffer.size</name>
  <value>8388608</value>
  </property>
  <property>
  <name>dfs.namenode.lifeline.handler.ratio</name>
  <value>0.10</value>
  </property>
  <property>
  <name>dfs.internal.nameservices</name>
  <value>emr-cluster</value>
  </property>
  <property>
  <name>dfs.webhdfs.rest-csrf.browser-useragents-regex</name>
  <value>^Mozilla.*,^Opera.*</value>
  </property>
  <property>
  <name>dfs.namenode.blocks.per.postponedblocks.rescan</name>
  <value>10000</value>
  </property>
  <property>
  <name>dfs.datanode.max.transfer.threads</name>
  <value>4096</value>
  </property>
  <property>
  <name>dfs.webhdfs.enabled</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.webhdfs.user.provider.user.pattern</name>
  <value>^[A-Za-z_][A-Za-z0-9._-]*[$]?$</value>
  </property>
  <property>
  <name>dfs.client.use.datanode.hostname</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.encrypt.data.transfer</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.namenode.list.cache.directives.num.responses</name>
  <value>100</value>
  </property>
  <property>
  <name>dfs.webhdfs.rest-csrf.custom-header</name>
  <value>X-XSRF-HEADER</value>
  </property>
  <property>
  <name>dfs.client.datanode-restart.timeout</name>
  <value>30</value>
  </property>
  <property>
  <name>dfs.http.client.retry.policy.spec</name>
  <value>10000,6,60000,10</value>
  </property>
  <property>
  <name>dfs.client.cache.readahead</name>
  <value/>
  </property>
  <property>
  <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</name>
  <value>0.75f</value>
  </property>
  <property>
  <name>dfs.namenode.http-bind-host</name>
  <value>0.0.0.0</value>
  </property>
  <property>
  <name>dfs.encrypt.data.transfer.cipher.key.bitlength</name>
  <value>128</value>
  </property>
  <property>
  <name>dfs.namenode.safemode.extension</name>
  <value>30000</value>
  </property>
  <property>
  <name>dfs.namenode.checkpoint.check.period</name>
  <value>60</value>
  </property>
  <property>
  <name>nfs.rtmax</name>
  <value>1048576</value>
  </property>
  <property>
  <name>dfs.http.client.retry.policy.enabled</name>
  <value>false</value>
  </property>
  <property>
  <name>nfs.kerberos.principal</name>
  <value/>
  </property>
  <property>
  <name>dfs.block.local-path-access.user</name>
  <value/>
  </property>
  <property>
  <name>dfs.client-write-packet-size</name>
  <value>65536</value>
  </property>
  <property>
  <name>dfs.client.mmap.retry.timeout.ms</name>
  <value>300000</value>
  </property>
  <property>
  <name>dfs.namenode.http-address.emr-cluster.nn2</name>
  <value>172.16.220.102:50070</value>
  </property>
  <property>
  <name>dfs.datanode.readahead.bytes</name>
  <value>4194304</value>
  </property>
  <property>
  <name>dfs.client.file-block-storage-locations.num-threads</name>
  <value>10</value>
  </property>
  <property>
  <name>dfs.image.compression.codec</name>
  <value>org.apache.hadoop.io.compress.DefaultCodec</value>
  </property>
  <property>
  <name>dfs.datanode.directoryscan.interval</name>
  <value>21600</value>
  </property>
  <property>
  <name>dfs.heartbeat.interval</name>
  <value>3</value>
  </property>
  <property>
  <name>dfs.datanode.slow.io.warning.threshold.ms</name>
  <value>300</value>
  </property>
  <property>
  <name>dfs.namenode.decommission.blocks.per.interval</name>
  <value>500000</value>
  </property>
  <property>
  <name>dfs.datanode.cache.revocation.polling.ms</name>
  <value>500</value>
  </property>
  <property>
  <name>dfs.datanode.data.dir.perm</name>
  <value>755</value>
  </property>
  <property>
  <name>dfs.namenode.top.enabled</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.ha.namenode.id</name>
  <value/>
  </property>
  <property>
  <name>dfs.datanode.kerberos.principal</name>
  <value/>
  </property>
  <property>
  <name>dfs.ha.log-roll.period</name>
  <value>120</value>
  </property>
  <property>
  <name>dfs.namenode.servicerpc-address.emr-cluster.nn1</name>
  <value>172.16.220.100:8021</value>
  </property>
  <property>
  <name>dfs.namenode.write-lock-reporting-threshold-ms</name>
  <value>5000</value>
  </property>
  <property>
  <name>dfs.namenode.servicerpc-address.emr-cluster.nn2</name>
  <value>172.16.220.102:8021</value>
  </property>
  <property>
  <name>dfs.namenode.kerberos.principal.pattern</name>
  <value>*</value>
  </property>
  <property>
  <name>dfs.namenode.write.stale.datanode.ratio</name>
  <value>0.5f</value>
  </property>
  <property>
  <name>dfs.namenode.metrics.logger.period.seconds</name>
  <value>600</value>
  </property>
  <property>
  <name>dfs.data.transfer.protection</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.resource.du.reserved</name>
  <value>1073741824</value>
  </property>
  <property>
  <name>dfs.namenode.safemode.replication.min</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.enable.retrycache</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.datanode.drop.cache.behind.writes</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.balancer.max-iteration-time</name>
  <value>1200000</value>
  </property>
  <property>
  <name>dfs.client.mmap.enabled</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.namenode.startup.delay.block.deletion.sec</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.mover.max-no-move-interval</name>
  <value>60000</value>
  </property>
  <property>
  <name>dfs.namenode.replication.interval</name>
  <value>3</value>
  </property>
  <property>
  <name>dfs.datanode.fsdatasetcache.max.threads.per.volume</name>
  <value>4</value>
  </property>
  <property>
  <name>dfs.http.client.failover.sleep.max.millis</name>
  <value>15000</value>
  </property>
  <property>
  <name>dfs.namenode.audit.loggers</name>
  <value>default</value>
  </property>
  <property>
  <name>dfs.trustedchannel.resolver.class</name>
  <value/>
  </property>
  <property>
  <name>dfs.client.block.write.retries</name>
  <value>3</value>
  </property>
  <property>
  <name>dfs.namenode.delegation.key.update-interval</name>
  <value>86400000</value>
  </property>
  <property>
  <name>dfs.client.failover.proxy.provider.emr-cluster</name>
  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
  <name>dfs.block.scanner.volume.bytes.per.second</name>
  <value>1048576</value>
  </property>
  <property>
  <name>dfs.http.client.retry.max.attempts</name>
  <value>10</value>
  </property>
  <property>
  <name>dfs.data.transfer.saslproperties.resolver.class</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.decommission.max.concurrent.tracked.nodes</name>
  <value>100</value>
  </property>
  <property>
  <name>dfs.namenode.checkpoint.edits.dir</name>
  <value>${dfs.namenode.checkpoint.dir}</value>
  </property>
  <property>
  <name>dfs.namenode.http-address.emr-cluster.nn1</name>
  <value>172.16.220.100:50070</value>
  </property>
  <property>
  <name>dfs.reformat.disabled</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.namenode.inode.attributes.provider.class</name>
  <value>org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer</value>
  </property>
  <property>
  <name>dfs.namenode.edekcacheloader.initial.delay.ms</name>
  <value>3000</value>
  </property>
  <property>
  <name>fs.oss.read.prefetch.count</name>
  <value>-1</value>
  </property>
  <property>
  <name>fs.oss.impl</name>
  <value>com.aliyun.emr.fs.oss.OssFileSystem</value>
  </property>
  <property>
  <name>dfs.datanode.shared.file.descriptor.paths</name>
  <value>/dev/shm,/tmp</value>
  </property>
  <property>
  <name>dfs.bytes-per-checksum</name>
  <value>512</value>
  </property>
  <property>
  <name>dfs.client.mmap.cache.timeout.ms</name>
  <value>3600000</value>
  </property>
  <property>
  <name>dfs.webhdfs.socket.connect-timeout</name>
  <value>60s</value>
  </property>
  <property>
  <name>nfs.keytab.file</name>
  <value/>
  </property>
  <property>
  <name>dfs.blocksize</name>
  <value>134217728</value>
  </property>
  <property>
  <name>dfs.namenode.num.extra.edits.retained</name>
  <value>1000000</value>
  </property>
  <property>
  <name>dfs.namenode.max.extra.edits.segments.retained</name>
  <value>10000</value>
  </property>
  <property>
  <name>ha.zookeeper.quorum</name>
  <value>172.16.220.103,172.16.220.102,172.16.220.100</value>
  </property>
  <property>
  <name>dfs.namenode.stale.datanode.interval</name>
  <value>30000</value>
  </property>
  <property>
  <name>dfs.client.failover.max.attempts</name>
  <value>15</value>
  </property>
  <property>
  <name>dfs.nameservices</name>
  <value>emr-cluster</value>
  </property>
  <property>
  <name>dfs.client.cache.drop.behind.writes</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.rpc-address.emr-cluster.nn1</name>
  <value>172.16.220.100:8020</value>
  </property>
  <property>
  <name>dfs.client.block.write.replace-datanode-on-failure.best-effort</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.namenode.rpc-address.emr-cluster.nn2</name>
  <value>172.16.220.102:8020</value>
  </property>
  <property>
  <name>dfs.image.transfer-bootstrap-standby.bandwidthPerSec</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.image.compress</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.xframe.enabled</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.hosts</name>
  <value/>
  </property>
  <property>
  <name>dfs.http.client.failover.sleep.base.millis</name>
  <value>500</value>
  </property>
  <property>
  <name>dfs.namenode.replication.max-streams-hard-limit</name>
  <value>100</value>
  </property>
  <property>
  <name>dfs.namenode.num.checkpoints.retained</name>
  <value>2</value>
  </property>
  <property>
  <name>dfs.ha.fencing.methods</name>
  <value>shell(/bin/true)</value>
  </property>
  <property>
  <name>dfs.xframe.value</name>
  <value>SAMEORIGIN</value>
  </property>
  <property>
  <name>dfs.replication</name>
  <value>2</value>
  </property>
  <property>
  <name>dfs.datanode.use.datanode.hostname</name>
  <value>false</value>
  </property>
  <property>
  <name>httpfs.buffer.size</name>
  <value>4096</value>
  </property>
  <property>
  <name>nfs.mountd.port</name>
  <value>4242</value>
  </property>
  <property>
  <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
  <value>DEFAULT</value>
  </property>
  <property>
  <name>dfs.client.read.shortcircuit.streams.cache.size</name>
  <value>256</value>
  </property>
  <property>
  <name>dfs.web.authentication.kerberos.principal</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.retrycache.heap.percent</name>
  <value>0.03f</value>
  </property>
  <property>
  <name>dfs.balancer.block-move.timeout</name>
  <value>600000</value>
  </property>
  <property>
  <name>hadoop.hdfs.configuration.version</name>
  <value>1</value>
  </property>
  <property>
  <name>dfs.http.client.failover.max.attempts</name>
  <value>15</value>
  </property>
  <property>
  <name>dfs.http.address</name>
  <value>0.0.0.0:50070</value>
  </property>
  <property>
  <name>dfs.permissions.superusergroup</name>
  <value>hadoop</value>
  </property>
  <property>
  <name>dfs.namenode.legacy-oiv-image.dir</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.edit.log.autoroll.check.interval.ms</name>
  <value>300000</value>
  </property>
  <property>
  <name>dfs.hosts.exclude</name>
  <value>/etc/ecm/hadoop-conf/dfs.exclude</value>
  </property>
  <property>
  <name>dfs.datanode.metrics.logger.period.seconds</name>
  <value>600</value>
  </property>
  <property>
  <name>dfs.namenode.checkpoint.period</name>
  <value>3600</value>
  </property>
  <property>
  <name>dfs.web.authentication.kerberos.keytab</name>
  <value/>
  </property>
  <property>
  <name>dfs.client.read.shortcircuit.skip.checksum</name>
  <value>false</value>
  </property>
  <property>
  <name>fs.oss.buffer.dirs</name>
  <value>file:///mnt/disk1/data</value>
  </property>
  <property>
  <name>dfs.namenode.path.based.cache.block.map.allocation.percent</name>
  <value>0.25</value>
  </property>
  <property>
  <name>dfs.image.transfer.chunksize</name>
  <value>65536</value>
  </property>
  <property>
  <name>dfs.support.append</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.namenode.top.num.users</name>
  <value>10</value>
  </property>
  <property>
  <name>dfs.namenode.top.window.num.buckets</name>
  <value>10</value>
  </property>
  <property>
  <name>dfs.stream-buffer-size</name>
  <value>4096</value>
  </property>
  <property>
  <name>dfs.namenode.edekcacheloader.interval.ms</name>
  <value>1000</value>
  </property>
  <property>
  <name>dfs.namenode.safemode.threshold-pct</name>
  <value>0.999f</value>
  </property>
  <property>
  <name>dfs.datanode.directoryscan.threads</name>
  <value>1</value>
  </property>
  <property>
  <name>dfs.namenode.checkpoint.max-retries</name>
  <value>3</value>
  </property>
  <property>
  <name>dfs.namenode.keytab.file</name>
  <value/>
  </property>
  <property>
  <name>dfs.client.use.legacy.blockreader.local</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.client.read.shortcircuit</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.cachereport.intervalMsec</name>
  <value>10000</value>
  </property>
  <property>
  <name>dfs.datanode.drop.cache.behind.reads</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.namenode.delegation.token.max-lifetime</name>
  <value>604800000</value>
  </property>
  <property>
  <name>dfs.client.failover.sleep.max.millis</name>
  <value>15000</value>
  </property>
  <property>
  <name>dfs.ha.automatic-failover.enabled</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.namenode.resource.checked.volumes.minimum</name>
  <value>1</value>
  </property>
  <property>
  <name>dfs.namenode.edits.dir</name>
  <value>file:///mnt/disk1/hdfs/edits</value>
  </property>
  <property>
  <name>dfs.metrics.percentiles.intervals</name>
  <value/>
  </property>
  <property>
  <name>dfs.journalnode.edits.dir</name>
  <value>/mnt/disk1/hdfs/journal</value>
  </property>
  <property>
  <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name>
  <value>${dfs.web.authentication.kerberos.principal}</value>
  </property>
  <property>
  <name>dfs.namenode.plugins</name>
  <value/>
  </property>
  <property>
  <name>hadoop.fuse.connection.timeout</name>
  <value>300</value>
  </property>
  <property>
  <name>dfs.namenode.checkpoint.dir</name>
  <value>file:///mnt/disk1/hdfs/namesecondary</value>
  </property>
  <property>
  <name>dfs.namenode.list.cache.pools.num.responses</name>
  <value>100</value>
  </property>
  <property>
  <name>dfs.image.transfer.timeout</name>
  <value>60000</value>
  </property>
  <property>
  <name>dfs.ha.namenodes.EXAMPLENAMESERVICE</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.fs-limits.max-xattrs-per-inode</name>
  <value>32</value>
  </property>
  <property>
  <name>dfs.balancer.kerberos.principal</name>
  <value/>
  </property>
  <property>
  <name>mapreduce.job.acl-view-job</name>
  <value>*</value>
  </property>
  <property>
  <name>dfs.permissions.enabled</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.client.write.exclude.nodes.cache.expiry.interval.millis</name>
  <value>600000</value>
  </property>
  <property>
  <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</name>
  <value>10737418240</value>
  </property>
  <property>
  <name>dfs.datanode.failed.volumes.tolerated</name>
  <value>3</value>
  </property>
  <property>
  <name>dfs.domain.socket.path</name>
  <value>/var/lib/hadoop-hdfs/dn_socket</value>
  </property>
  <property>
  <name>dfs.namenode.read-lock-reporting-threshold-ms</name>
  <value>5000</value>
  </property>
  <property>
  <name>dfs.namenode.delegation.token.renew-interval</name>
  <value>86400000</value>
  </property>
  <property>
  <name>dfs.client.socket.send.buffer.size</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.namenode.quota.init-threads</name>
  <value>4</value>
  </property>
  <property>
  <name>dfs.namenode.https-bind-host</name>
  <value>0.0.0.0</value>
  </property>
  <property>
  <name>dfs.namenode.top.windows.minutes</name>
  <value>1,5,25</value>
  </property>
  <property>
  <name>dfs.namenode.edits.asynclogging</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.namenode.rpc-bind-host</name>
  <value>0.0.0.0</value>
  </property>
  <property>
  <name>dfs.datanode.transfer.socket.send.buffer.size</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.client.local.interfaces</name>
  <value/>
  </property>
  <property>
  <name>dfs.ha.namenodes.emr-cluster</name>
  <value>nn1,nn2</value>
  </property>
  <property>
  <name>dfs.datanode.cache.revocation.timeout.ms</name>
  <value>900000</value>
  </property>
  <property>
  <name>dfs.client.slow.io.warning.threshold.ms</name>
  <value>30000</value>
  </property>
  <property>
  <name>dfs.namenode.max.full.block.report.leases</name>
  <value>6</value>
  </property>
  <property>
  <name>dfs.storage.policy.enabled</name>
  <value>true</value>
  </property>
  <property>
  <name>dfs.client.failover.connection.retries</name>
  <value>0</value>
  </property>
  <property>
  <name>dfs.namenode.list.encryption.zones.num.responses</name>
  <value>100</value>
  </property>
  <property>
  <name>dfs.client.domain.socket.data.traffic</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.datanode.block.id.layout.upgrade.threads</name>
  <value>12</value>
  </property>
  <property>
  <name>dfs.datanode.lifeline.interval.seconds</name>
  <value/>
  </property>
  <property>
  <name>dfs.namenode.full.block.report.lease.length.ms</name>
  <value>300000</value>
  </property>
  <property>
  <name>dfs.client.file-block-storage-locations.timeout.millis</name>
  <value>60000</value>
  </property>
  <property>
  <name>dfs.client.context</name>
  <value>default</value>
  </property>
  <property>
  <name>dfs.datanode.scan.period.hours</name>
  <value>504</value>
  </property>
  <property>
  <name>dfs.datanode.sync.behind.writes</name>
  <value>false</value>
  </property>
  <property>
  <name>dfs.balancer.address</name>
  <value>0.0.0.0:0</value>
  </property>
  <property>
  <name>dfs.datanode.bp-ready.timeout</name>
  <value>20</value>
  </property>
  </configuration>


  hive-site.xml: |+
    <?xml version="1.0"?>
  <configuration>
  <property>
  <name>hive.merge.smallfiles.avgsize</name>
  <value>16000000</value>
  </property>
  <property>
  <name>hive.tez.container.size</name>
  <value>4096</value>
  </property>
  <property>
  <name>hive.exec.max.created.files</name>
  <value>100000</value>
  </property>
  <property>
  <name>hive.execution.engine</name>
  <value>mr</value>
  </property>
  <property>
  <name>hive.map.aggr.hash.min.reduction</name>
  <value>0.5</value>
  </property>
  <property>
  <name>hive.skewjoin.mapjoin.map.tasks</name>
  <value>10000</value>
  </property>
  <property>
  <name>hive.merge.tezfiles</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.compactor.worker.threads</name>
  <value>0</value>
  </property>
  <property>
  <name>hive.exec.parallel.thread.number</name>
  <value>8</value>
  </property>
  <property>
  <name>hive.mapjoin.smalltable.filesize</name>
  <value>25000000</value>
  </property>
  <property>
  <name>hive.merge.sparkfiles</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.map.aggr.hash.percentmemory</name>
  <value>0.5</value>
  </property>
  <property>
  <name>hive.tez.cpu.vcores</name>
  <value>-1</value>
  </property>
  <property>
  <name>hive.aux.jars.path</name>
  <value/>
  </property>
  <property>
  <name>hive.groupby.skewindata</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.ignore.mapjoin.hint</name>
  <value>true</value>
  </property>
  <property>
  <name>hive.optimize.dynamic.partition.hashjoin</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.compactor.initiator.on</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.security.metastore.authenticator.manager</name>
  <value>org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator</value>
  </property>
  <property>
  <name>hive.spark.client.server.connect.timeout</name>
  <value>600000</value>
  </property>
  <property>
  <name>hive.exec.scratchdir</name>
  <value>/tmp/hive</value>
  </property>
  <property>
  <name>hive.exec.post.hooks</name>
  <value>org.apache.hadoop.hive.ql.hooks.LineageLogger,com.aliyun.emr.meta.hive.hook.LineageLoggerHook,org.apache.atlas.hive.hook.HiveHook</value>
  </property>
  <property>
  <name>hive.stats.autogather</name>
  <value>true</value>
  </property>
  <property>
  <name>init.meta.db</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.cbo.enable</name>
  <value>true</value>
  </property>
  <property>
  <name>hive.metastore.uris</name>
  <value>thrift://172.16.220.100:9083,thrift://172.16.220.102:9083</value>
  </property>
  <property>
  <name>hive.downloaded.resources.dir</name>
  <value>/tmp/${hive.session.id}_resources</value>
  </property>
  <property>
  <name>hive.skewjoin.mapjoin.min.split</name>
  <value>33554432</value>
  </property>
  <property>
  <name>hive.server2.support.dynamic.service.discovery</name>
  <value>true</value>
  </property>
  <property>
  <name>hive.exec.max.dynamic.partitions</name>
  <value>1000</value>
  </property>
  <property>
  <name>hive.exec.reducers.bytes.per.reducer</name>
  <value>256000000</value>
  </property>
  <property>
  <name>hive.exec.dynamic.partition</name>
  <value>true</value>
  </property>
  <property>
  <name>hive.optimize.skewjoin</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.txn.manager</name>
  <value>org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager</value>
  </property>
  <property>
  <name>hive.metastore.warehouse.dir</name>
  <value>/user/hive/warehouse</value>
  </property>
  <property>
  <name>hive.jobname.length</name>
  <value>50</value>
  </property>
  <property>
  <name>hive.exec.mode.local.auto</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.server2.zookeeper.namespace</name>
  <value>hiveserver2</value>
  </property>
  <property>
  <name>hive.exec.mode.local.auto.input.files.max</name>
  <value>4</value>
  </property>
  <property>
  <name>hive.exec.compress.output</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.metastore.schema.verification</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.merge.size.per.task</name>
  <value>256000000</value>
  </property>
  <property>
  <name>hive.auto.convert.join</name>
  <value>true</value>
  </property>
  <property>
  <name>hive.exec.reducers.max</name>
  <value>1009</value>
  </property>
  <property>
  <name>hive.server2.thrift.max.worker.threads</name>
  <value>1000</value>
  </property>
  <property>
  <name>hive.mapred.reduce.tasks.speculative.execution</name>
  <value>true</value>
  </property>
  <property>
  <name>hive.map.aggr</name>
  <value>true</value>
  </property>
  <property>
  <name>hive.support.concurrency</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.tez.java.opts</name>
  <value>-Xmx3800m</value>
  </property>
  <property>
  <name>hive.merge.mapredfiles</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.metastore.client.socket.timeout</name>
  <value>600s</value>
  </property>
  <property>
  <name>hive.jar.path</name>
  <value/>
  </property>
  <property>
  <name>hive.skewjoin.key</name>
  <value>100000</value>
  </property>
  <property>
  <name>hive.exec.max.dynamic.partitions.pernode</name>
  <value>100</value>
  </property>
  <property>
  <name>hive.zookeeper.quorum</name>
  <value>172.16.220.101,172.16.220.102,172.16.220.100</value>
  </property>
  <property>
  <name>hive.stats.column.autogather</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.exec.parallel</name>
  <value>false</value>
  </property>
  <property>
  <name>hive.map.aggr.hash.force.flush.memory.threshold</name>
  <value>0.9</value>
  </property>
  <property>
  <name>hive.exec.mode.local.auto.inputbytes.max</name>
  <value>134217728</value>
  </property>
  <property>
  <name>hive.security.metastore.authorization.manager</name>
  <value>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider</value>
  </property>
  <property>
  <name>hive.exec.dynamic.partition.mode</name>
  <value>nonstrict</value>
  </property>
  <property>
  <name>hive.metastore.pre.event.listeners</name>
  <value>org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener</value>
  </property>
  <property>
  <name>hive.merge.mapfiles</name>
  <value>true</value>
  </property>
  <property>
  <name>hive.mapjoin.followby.map.aggr.hash.percentmemory</name>
  <value>0.3</value>
  </property>
  </configuration>

